{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggNqjELRQrkX"
      },
      "source": [
        "## Download DOI collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWj4bHmVPYAz",
        "outputId": "eab7c7ff-55b8-4b90-fab8-9e57f9cb256c"
      },
      "outputs": [],
      "source": [
        "!curl -O https://raw.githubusercontent.com/BadSoyo/paper_crawler/refs/heads/main/electrolyte-brain-doi-0120/downloaded_doi_0120.json\n",
        "!curl -O https://raw.githubusercontent.com/BadSoyo/paper_crawler/refs/heads/main/electrolyte-brain-doi-0120/press-config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rjE1VtLQ4YH"
      },
      "source": [
        "## Install dependence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezTHGeTCP1j0",
        "outputId": "2a084da0-89dc-4186-e1a6-9efc7509cf8d"
      },
      "outputs": [],
      "source": [
        "!pip install minio\n",
        "!pip install habanero\n",
        "!pip install bs4\n",
        "!pip install html5lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kil0DPhSPvxD"
      },
      "outputs": [],
      "source": [
        "import minio\n",
        "from habanero import Crossref\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import html5lib\n",
        "from google.colab import userdata\n",
        "import gzip\n",
        "import io\n",
        "import json\n",
        "\n",
        "client = minio.Minio('minio.hzc.pub', access_key=userdata.get('AKA'), secret_key=userdata.get('SK'))\n",
        "buckets = client.list_buckets()\n",
        "\n",
        "cr = Crossref()\n",
        "\n",
        "with open(\"press-config.json\", \"r\") as f:\n",
        "  press_config = json.load(f)\n",
        "  press_config[\"10.1006\"] = press_config[\"10.1016\"]\n",
        "  press_config[\"10.1149\"] = press_config[\"10.1088\"]\n",
        "\n",
        "def save_html(text, doi):\n",
        "  # testing\n",
        "  with open(f'{doi.replace(\"/\", \"_\")}.html', 'w', encoding='utf-8') as f:\n",
        "    f.write(text.decode('utf-8'))\n",
        "\n",
        "def remove_html_tags_with_bs4(text):\n",
        "  soup_tmp = BeautifulSoup(text, 'html.parser')\n",
        "  return soup_tmp.get_text().strip()\n",
        "\n",
        "def get_html_content(doi):\n",
        "  try:\n",
        "      response = client.get_object('electrolyte-brain', f\"{doi}/_.html.gz\")\n",
        "\n",
        "      with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as gz_file:\n",
        "          file_content = gz_file.read().decode('utf-8')  # 假设文件编码为 utf-8\n",
        "      return file_content  # 打印文件内容字符串\n",
        "  finally:\n",
        "      response.close()\n",
        "      response.release_conn()\n",
        "\n",
        "def get_selector(doi):\n",
        "  press = doi.split(\"/\")[0]\n",
        "  sel_A = press_config.get(press, {}).get(\"sel_A\") # abstract\n",
        "  sel_R = press_config.get(press, {}).get(\"sel_R\") # references\n",
        "  sel_P = press_config.get(press, {}).get(\"sel_P\") # paragraphs\n",
        "  sel_F = press_config.get(press, {}).get(\"sel_F\") # figures\n",
        "  sel_S = press_config.get(press, {}).get(\"sel_S\") # schemes\n",
        "  sel_T = press_config.get(press, {}).get(\"sel_T\") # tables\n",
        "\n",
        "  return sel_A, sel_R, sel_P, sel_F, sel_S, sel_T\n",
        "\n",
        "\n",
        "def get_doi_detail(doi):\n",
        "    result = cr.works(ids=doi.replace(\"_\",\"/\"), format = \"citeproc-json\")\n",
        "    title = result.get(\"message\", {}).get(\"title\", None)\n",
        "    container = result.get(\"message\", {}).get(\"container-title\", None)\n",
        "    # short_container = result.get(\"message\", {}).get(\"short-container-title\", None)\n",
        "    author = result.get(\"message\", {}).get(\"author\", None)\n",
        "    date = result.get(\"message\", {}).get(\"published\", {}).get(\"date-parts\", None)[0]\n",
        "    cite_count = result.get(\"message\", {}).get(\"is-referenced-by-count\", None)\n",
        "    abstract = result.get(\"message\", {}).get(\"abstract\", None)\n",
        "    # volume = result.get(\"message\", {}).get(\"volume\", None)\n",
        "    # page = result.get(\"message\", {}).get(\"page\", None)\n",
        "    press = doi.split(\"/\")[0]\n",
        "\n",
        "    html_content = get_html_content(doi)\n",
        "    result = html5lib.serialize(html5lib.parse(html_content), encoding=\"utf-8\", omit_optional_tags=False)\n",
        "    # save_html(result, doi)\n",
        "    soup = BeautifulSoup(result, 'html.parser')\n",
        "\n",
        "    abstract_selector, ref_selector, paragraph_selector, figure_selector, scheme_selector, table_selector = get_selector(doi)\n",
        "\n",
        "    for selector in abstract_selector:\n",
        "        # if abstract:\n",
        "        #     break\n",
        "        abstract_el = soup.select_one(selector)\n",
        "        if not abstract_el:\n",
        "            continue\n",
        "        abstract = abstract_el.text.strip()\n",
        "\n",
        "\n",
        "    paragraphs = []\n",
        "    for selector in paragraph_selector:\n",
        "        if paragraphs:\n",
        "            break\n",
        "        paragraph_els = soup.select(selector)\n",
        "        if not paragraph_els:\n",
        "            continue\n",
        "        for p in paragraph_els:\n",
        "            source = p.decode()\n",
        "            if ref_selector and ref_selector.strip():\n",
        "                refs = [ref.extract() for ref in p.select(ref_selector)]\n",
        "            else:\n",
        "                refs = []\n",
        "            paragraphs.append({\n",
        "                # \"source\": source,\n",
        "                \"text\": p.text.strip(),\n",
        "                \"refs\": list(set([ref.text.strip() for ref in refs])),\n",
        "            })\n",
        "\n",
        "    figures = []\n",
        "    for selector in figure_selector:\n",
        "        if figures:\n",
        "            break\n",
        "        figure_els = soup.select(selector)\n",
        "        # print(\"figure_els:\", soup.select(\".ArticleDetails__main .FigureDesc\"))\n",
        "        if not figure_els:\n",
        "            continue\n",
        "        for f in figure_els:\n",
        "            f_parent = None\n",
        "            if press == \"10.1002\":\n",
        "              f_parent = f.parent\n",
        "              f_extra = f_parent.select_one(\".figure-extra\")\n",
        "              if f_extra:\n",
        "                f_extra.extract()\n",
        "            figures.append((f_parent or f).text.strip().replace(\"\\n\", \" \")),\n",
        "\n",
        "\n",
        "    schemes = []\n",
        "    for selector in scheme_selector:\n",
        "        if schemes:\n",
        "            break\n",
        "        scheme_els = soup.select(selector)\n",
        "        if not scheme_els:\n",
        "            continue\n",
        "        for f in scheme_els:\n",
        "            f_parent = None\n",
        "            if press == \"10.1002\":\n",
        "              f_parent = f.parent\n",
        "              # print(f_parent)\n",
        "              f_extra = f_parent.select_one(\".figure-extra\")\n",
        "              if f_extra:\n",
        "                f_extra.extract()\n",
        "            schemes.append((f_parent or f).text.strip().replace(\"\\n\", \" \")),\n",
        "\n",
        "    tables = [\n",
        "        # {\n",
        "        #   # \"title\": [],\n",
        "        #   # \"table\": [],\n",
        "        #   # \"footer\": [],\n",
        "        # }\n",
        "    ]\n",
        "    for selectorObj in table_selector:\n",
        "        if len(tables) > 0:\n",
        "            break\n",
        "        wraper_selector = selectorObj.get(\"wrapper\")\n",
        "        title_selector = selectorObj.get(\"title\")\n",
        "        table_selector = selectorObj.get(\"table\")\n",
        "        footer_selector = selectorObj.get(\"footer\")\n",
        "        if not (wraper_selector or title_selector or table_selector or footer_selector):\n",
        "            continue\n",
        "        if not footer_selector: # and not table_selector:\n",
        "          table_title_els = soup.select(f'{wraper_selector} {title_selector}')\n",
        "          if not table_title_els:\n",
        "            continue\n",
        "          for t in table_title_els:\n",
        "            title_text = t.text.strip().replace(\"\\n\", \" \")\n",
        "            tables.append({\"title\": title_text}),\n",
        "          continue\n",
        "        else:\n",
        "          table_wrap_els = soup.select(wraper_selector)\n",
        "          if not table_wrap_els:\n",
        "            continue\n",
        "          for t in table_wrap_els:\n",
        "            table_title_el = t.select_one(title_selector)\n",
        "            # table_body_el = t.select_one(table_selector)\n",
        "            # Edge case for 10.3762\n",
        "            if press in ['10.3762']:\n",
        "              table_footer_el = t.parent.select_one(\":scope > p\")\n",
        "            elif type(footer_selector) is list:\n",
        "              table_footer_el = t.select_one(footer_selector[0])\n",
        "              if not table_footer_el:\n",
        "                table_footer_el = t.select_one(footer_selector[1])\n",
        "            else:\n",
        "              table_footer_el = t.select_one(footer_selector)\n",
        "            if not table_title_el and not table_footer_el:\n",
        "              continue\n",
        "            tables.append({\n",
        "              \"title\": table_title_el and table_title_el.text.strip().replace(\"\\n\", \" \"),\n",
        "              # \"table\": table_body_el.decode(),\n",
        "              \"footer\": table_footer_el and table_footer_el.text.strip().replace(\"\\n\", \" \"),\n",
        "            })\n",
        "\n",
        "\n",
        "    # recognize by text\n",
        "    if press in [\"10.1016\", \"10.1006\", '10.3390', \"10.1002\"]:\n",
        "      figures = [f for f in figures if f.lower().strip().startswith('fig')]\n",
        "      schemes = [f for f in schemes if f.lower().strip().startswith('sch')]\n",
        "\n",
        "    if press in [\"10.3389\", \"10.1371\"]:\n",
        "      figures = [f for f in figures if f.lower().strip().startswith('fig')]\n",
        "      tables = [f for f in tables if f[\"title\"].lower().strip().startswith('tab')]\n",
        "\n",
        "    return {\n",
        "        \"title\": remove_html_tags_with_bs4(title[0]),\n",
        "        \"journalName\": container[0],\n",
        "        \"authors\": [f'{item.get(\"given\", \"\")} {item.get(\"family\", \"\")}' for item in author],\n",
        "        \"pubDate\": \"-\".join([str(item) for item in date]),\n",
        "        \"citations\": cite_count,\n",
        "        \"doi\": doi,\n",
        "        \"abstract\": abstract,\n",
        "        \"paragraphs\": paragraphs and [item.get(\"text\", \"\") for item in paragraphs],\n",
        "        \"figureCaptions\": figures,\n",
        "        \"schemeCaptions\": schemes,\n",
        "        \"tables\": tables,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOvt24sCRLgP"
      },
      "source": [
        "## Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqAgbHF5P7TB",
        "outputId": "48bedcf4-8c79-4166-f4ed-47441e3b6495"
      },
      "outputs": [],
      "source": [
        "with open('downloaded_doi_0120.json', 'r') as f:\n",
        "    dois = json.load(f)\n",
        "filter_dois = dois #[doi for doi in dois if doi.startswith(\"10.1088\")]\n",
        "print(f'total num: {len(filter_dois)}')\n",
        "import random\n",
        "random_integer = random.randint(1,len(filter_dois)-1)\n",
        "print(random_integer)\n",
        "get_doi_detail(filter_dois[random_integer])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print all DOI prefix types.\n",
        "with open(\"downloaded_doi_0120.json\", \"r\") as f:\n",
        "    all_dois = json.load(f)\n",
        "unique_prefixes = set()\n",
        "for doi in all_dois:\n",
        "    prefix = doi.split(\"/\")[0]\n",
        "    unique_prefixes.add(prefix)\n",
        "\n",
        "print(list(unique_prefixes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform sample testing across all DOI prefix types.\n",
        "successful_details = []\n",
        "failed_dois_by_prefix = {}\n",
        "\n",
        "# Convert unique_prefixes to a list to iterate if it's still a set\n",
        "if isinstance(unique_prefixes, set):\n",
        "    unique_prefixes_list = list(unique_prefixes)\n",
        "else:\n",
        "    unique_prefixes_list = unique_prefixes\n",
        "\n",
        "for prefix in unique_prefixes_list:\n",
        "    dois_for_prefix = [doi for doi in all_dois if doi.startswith(prefix)]\n",
        "\n",
        "    if dois_for_prefix:\n",
        "        selected_doi = random.choice(dois_for_prefix)\n",
        "        print(\n",
        "            f\"\\nAttempting to retrieve details for random DOI: {selected_doi} (Prefix: {prefix})\"\n",
        "        )\n",
        "        try:\n",
        "            detail = get_doi_detail(selected_doi)\n",
        "            successful_details.append(detail)\n",
        "            print(f\"Successfully retrieved details for {selected_doi}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving details for {selected_doi}: {e}\")\n",
        "            if prefix not in failed_dois_by_prefix:\n",
        "                failed_dois_by_prefix[prefix] = []\n",
        "            failed_dois_by_prefix[prefix].append({\"doi\": selected_doi, \"error\": str(e)})\n",
        "    else:\n",
        "        print(f\"No DOIs found for prefix: {prefix}\")\n",
        "\n",
        "print(\"\\n--- Summary ---\")\n",
        "print(f\"Total successful DOI retrievals: {len(successful_details)}\")\n",
        "print(f\"Prefixes with errors: {len(failed_dois_by_prefix)}\")\n",
        "for prefix, failures in failed_dois_by_prefix.items():\n",
        "    print(\n",
        "        f\"  Prefix {prefix} failed for {len(failures)} DOI(s). Example error: {failures[0]['error']}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample and test DOIs with a specific prefix, then print the results.\n",
        "with open(\"downloaded_doi_0120.json\", \"r\") as f:\n",
        "    all_dois = json.load(f)\n",
        "\n",
        "target_prefix = \"10.15376\"\n",
        "\n",
        "filtered_dois = [doi for doi in all_dois if doi.startswith(target_prefix)]\n",
        "\n",
        "print(f'Total number of DOIs with prefix \"{target_prefix}\": {len(filtered_dois)}')\n",
        "\n",
        "if not filtered_dois:\n",
        "    print(f'No DOIs found with prefix \"{target_prefix}\".')\n",
        "else:\n",
        "    random_integer = random.randint(0, len(filtered_dois) - 1)\n",
        "    selected_doi = filtered_dois[random_integer]\n",
        "    print(f'Randomly selected DOI from prefix \"{target_prefix}\" list: {selected_doi}')\n",
        "get_doi_detail(selected_doi)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
